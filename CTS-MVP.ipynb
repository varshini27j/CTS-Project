{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain openai gradio chromadb tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import openai\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "import chromadb\n",
    "import chromadb.config\n",
    "\n",
    "import tiktoken\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "PERSIST = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = None\n",
    "\n",
    "\n",
    "def main_func(message , history):\n",
    "  chat_history = []\n",
    "  if PERSIST and os.path.exists(\"persist\"):\n",
    "    print(\"Reusing index...\\n\")\n",
    "    vectorstore = Chroma(persist_directory=\"persist\", embedding_function=OpenAIEmbeddings())\n",
    "    index = VectorStoreIndexWrapper(vectorstore=vectorstore)\n",
    "  else:\n",
    "    loader = TextLoader(\"data.txt\")\n",
    "    if PERSIST:\n",
    "      index = VectorstoreIndexCreator(vectorstore_kwargs={\"persist_directory\":\"persist\"}).from_loaders([loader])\n",
    "    else:\n",
    "      index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "  print(index)\n",
    "\n",
    "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "  chain = ConversationalRetrievalChain.from_llm(llm=ChatOpenAI(), retriever=index.vectorstore.as_retriever(), memory=memory, verbose=True)\n",
    "\n",
    "  query = message\n",
    "  result = chain({\"question\": query, \"chat_history\": chat_history})\n",
    "  print(result['answer'])\n",
    "  chat_history.append((query, result['answer']))\n",
    "  return result['answer']\n",
    "\n",
    "gr.ChatInterface(main_func ,title=\"CHIT CHAT ðŸ¤–\",\n",
    "                 chatbot=gr.Chatbot(height=550),\n",
    "                 textbox=gr.Textbox(placeholder=\"Ask me anything about the artifacts\" ),\n",
    "\n",
    "                 ).launch())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
